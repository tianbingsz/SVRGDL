#!/usr/bin/env python
# Copyright (c) 2016 Baidu, Inc. All Rights Reserved


import math
import sys
import paddle.trainer.recurrent_units as recurrent
from paddle.trainer_config_helpers.data_sources import *

eos_id = 1
start_id = 0

generating = get_config_arg('generating', bool, False)
gen_trans_file = get_config_arg('gen_trans_file', str, None)
beam_size = get_config_arg('beam_size', int, 20)

print('generating : %d' % (generating))
if generating:
    print('gen_trans_file : %s' % (gen_trans_file))
#####################  Network Configuration ##############################################
dir = "./regression_tests/data/nmt/"

src_lang_dict = dir + "src.dict"
src_dict_dim = len(open(src_lang_dict, 'r').readlines())

trg_lang_dict = dir + "trg.dict"
dict_for_gen = trg_lang_dict
trg_dict_dim = len(open(trg_lang_dict, 'r').readlines())

word_vec_dim = 32
latent_chain_dim = 32
clipping_thd = 50
lr_hid_col = 8e-2

max_frames = 250
res_per_sample = beam_size

default_decay_rate(2e-3)
default_num_batches_regularization(1)
default_initial_std(1 / math.sqrt(latent_chain_dim * 3.0))
default_momentum(0.9)
default_initial_smart(True)

model_type("recurrent_nn")
src_dict = dict()
for line_count, line in enumerate(open(src_lang_dict, "r")):
    src_dict[line.strip()] = line_count
trg_dict = dict()
for line_count, line in enumerate(open(trg_lang_dict, "r")):
    trg_dict[line.strip()] = line_count

train_list = None if generating else dir + 'train.lst'
test_list = dir + 'gen.lst' if generating else dir + 'test.lst'
if generating:
    trg_dict = None

define_py_data_sources(train_list, test_list,
                       module = ["regression_tests.provider.nmt",
                                 "regression_tests.provider.nmt_test"],
                       obj = "process2",
                       args = {"src_dict": src_dict,
                               "trg_dict": trg_dict},
                       train_async = True)
if generating:
    Settings(algorithm='sgd', batch_size=1, learning_rate=0, )
else:
    Settings(algorithm='sgd',
             learning_method='adam',
             learning_rate=5e-2,
             do_average_in_cpu=True,
             learning_rate_decay_a=0,
             learning_rate_decay_b=0,
             ada_rou=0.95,
             ada_epsilon=1e-6,
             batch_size=50,
             average_window=0.5,
             max_average_window=2500 * 2,
             num_batches_per_send_parameter=1,
             num_batches_per_get_parameter=1, )

DataLayer(name="src_words", size=src_dict_dim, )

if generating:
    Inputs("src_words", "sent_id", )
    Outputs(
        "src_emb")  # dummuy but must be included in multi-thread generation
    Layer(name="sent_id", type="data", size=1, )
else:
    Inputs("src_words", "trg_words", "trg_next_words")
    Outputs("cost")

    DataLayer(name="trg_words", size=trg_dict_dim, )

    DataLayer(name="trg_next_words", size=trg_dict_dim, )

MixedLayer(name="src_emb",
           size=word_vec_dim,
           active_type="",
           bias=False,
           inputs=TableProjection("src_words",
                                  initial_std=1 / math.sqrt(word_vec_dim),
                                  parameter_name="_src_emb"), )

RecurrentLayerGroupBegin(
    name = "combine_multi_encoder",
    in_links = [Link("src_emb", has_subseq = True)],
    out_links = [Link("encoder_forward", has_subseq = True),\
        Link("encoder_backward", has_subseq = True)],
    seq_reversed = False,
)
for direction in ["forward", "backward"]:
    recurrent.GatedRecurrentLayerGroup(
        name="encoder_" + direction,
        size=latent_chain_dim,
        para_prefix="encoder_" + direction,
        active_type="tanh",
        gate_active_type="sigmoid",
        inputs=[FullMatrixProjection("src_emb")],
        error_clipping_threshold=clipping_thd,
        seq_reversed=0 if (direction == 'forward') else 1, )
RecurrentLayerGroupEnd("combine_multi_encoder")

Layer(name="encoder_forward_last",
      type="seqlastins",
      trans_type="seq",
      bias=False,
      inputs=[Input("encoder_forward")])
Layer(name="encoder_backward_first",
      type="seqfirstins",
      trans_type="seq",
      bias=False,
      inputs=[Input("encoder_backward")])

Layer(name="encoder",
      type="concat",
      inputs=["encoder_forward_last", "encoder_backward_first"])

MixedLayer(name="encoder_proj",
           size=latent_chain_dim,
           active_type="tanh",
           bias=False,
           inputs=FullMatrixProjection("encoder"), )

#################################### decoder #######################################
if not generating:
    MixedLayer(name="trg_emb",
               size=word_vec_dim,
               active_type="tanh",
               bias=False,
               inputs=TableProjection("trg_words",
                                      initial_std=1 / math.sqrt(word_vec_dim),
                                      parameter_name="_trg_emb"), )


RecurrentLayerGroupBegin(
    name = "decoder_layer_group",
    in_links = [] if generating else ["trg_emb"],
    out_links = ["predict_word"] if generating else ["output"],
    seq_reversed = False,
    generator = Generator(
        max_num_frames = max_frames,\
        beam_size = beam_size,\
        num_results_per_sample = beam_size) if generating else None,
)

if generating:
    predict_word_memory = Memory(name="predict_word",
                                 size=trg_dict_dim,
                                 boot_with_const_id=start_id, )
    MixedLayer(name="trg_emb",
               size=word_vec_dim,
               active_type="",
               bias=False,
               inputs=TableProjection(predict_word_memory,
                                      parameter_name="_trg_emb"), )

encoder_memory = Memory(name="encoder_read_memory",
                        boot_layer="encoder_proj",
                        boot_bias=False,
                        size=latent_chain_dim,
                        is_sequence=True, )
MixedLayer(name="encoder_read_memory",
           size=latent_chain_dim,
           active_type="",
           bias=False,
           inputs=IdentityProjection(encoder_memory), )

Layer(name="context_weights",
      type="mixed",
      size=1,
      active_type="sequence_softmax",
      bias=False,
      inputs=FullMatrixProjection(encoder_memory), )

Layer(name="context_vectors",
      type="scaling",
      inputs=["context_weights", encoder_memory], )

Layer(name="context",
      type="average",
      average_strategy="sum",
      inputs=["context_vectors"], )

recurrent.GatedRecurrentUnit(
    name="decoder",
    size=latent_chain_dim,
    active_type="tanh",
    gate_active_type="sigmoid",
    inputs=[FullMatrixProjection("context"), FullMatrixProjection("trg_emb")],
    para_prefix="decoder",
    error_clipping_threshold=clipping_thd,
    out_memory=None)

Layer(name="output",
      type="mixed",
      size=trg_dict_dim,
      active_type="softmax",
      bias=Bias(parameter_name="_output.wbias"),
      inputs=FullMatrixProjection("decoder",
                                  parameter_name="_output.w"), )
if generating:
    Layer(name="predict_word", type="maxid", inputs=["output"], )
    Layer(name="eos_check",
          type="eos_id",
          eos_id=eos_id,
          inputs=["predict_word"], )
RecurrentLayerGroupEnd("decoder_layer_group")

if generating:
    Evaluator(name="target_printer",
              type="seq_text_printer",
              dict_file=dict_for_gen,
              result_file=gen_trans_file,
              inputs=["sent_id", "predict_word"], )
else:
    Layer(name="cost",
          type="multi-class-cross-entropy",
          inputs=["output", "trg_next_words"], )
    Evaluator(name="token_error_rate",
              type="classification_error",
              inputs=["output", "trg_next_words"])
